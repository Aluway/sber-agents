import os
import logging
import time
from typing import Optional, Dict, Any, List
from langsmith import Client
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage
from langchain_core.outputs import LLMResult
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, BitsAndBytesConfig
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º pipeline –ª–µ–Ω–∏–≤–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å torchvision –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
from datasets import Dataset

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è HuggingFace Hub, –µ—Å–ª–∏ –æ–Ω –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
if 'HTTP_PROXY' not in os.environ and 'HTTPS_PROXY' not in os.environ:
    os.environ.setdefault('NO_PROXY', 'huggingface.co,*.huggingface.co')

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º accelerate –∑–∞—Ä–∞–Ω–µ–µ, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è device_map
try:
    import accelerate
    ACCELERATE_AVAILABLE = True
except ImportError:
    ACCELERATE_AVAILABLE = False
from ragas import evaluate
from ragas.metrics import (
    Faithfulness,
    ResponseRelevancy,
    AnswerCorrectness,
    AnswerSimilarity,
    ContextRecall,
    ContextPrecision,
)
from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.run_config import RunConfig
from config import config
import rag

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
_ragas_metrics = None
_ragas_run_config = None
_cached_provider = None  # –ö—ç—à–∏—Ä—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä, —á—Ç–æ–±—ã –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (rate limiting)
_last_request_time = 0.0

class RateLimitedLLM(BaseChatModel):
    """
    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è LLM —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è rate limit –æ—à–∏–±–æ–∫
    """
    def __init__(self, llm: BaseChatModel, delay: float = 2.0, **kwargs):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –±–µ–∑ –ø–µ—Ä–µ–¥–∞—á–∏ llm –∫–∞–∫ –ø–æ–ª—è –º–æ–¥–µ–ª–∏
        super().__init__(**kwargs)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º llm –∫–∞–∫ –æ–±—ã—á–Ω—ã–π –∞—Ç—Ä–∏–±—É—Ç (–Ω–µ pydantic –ø–æ–ª–µ)
        object.__setattr__(self, 'llm', llm)
        object.__setattr__(self, 'delay', delay)
        object.__setattr__(self, '_last_request_time', 0.0)
    
    def __getattr__(self, name):
        """–î–µ–ª–µ–≥–∏—Ä—É–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É LLM"""
        if name == 'llm':
            return object.__getattribute__(self, 'llm')
        try:
            return getattr(self.llm, name)
        except AttributeError:
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")
    
    def __setattr__(self, name, value):
        """–ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –¥–ª—è llm"""
        if name in ('llm', 'delay', '_last_request_time'):
            object.__setattr__(self, name, value)
        else:
            super().__setattr__(name, value)
    
    def _enforce_rate_limit(self):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        current_time = time.time()
        time_since_last = current_time - self._last_request_time
        if time_since_last < self.delay:
            sleep_time = self.delay - time_since_last
            logger.debug(f"Rate limiting: sleeping {sleep_time:.2f}s before next request")
            time.sleep(sleep_time)
        self._last_request_time = time.time()
    
    def invoke(self, input, config=None, **kwargs):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ —Å rate limiting"""
        self._enforce_rate_limit()
        return self.llm.invoke(input, config=config, **kwargs)
    
    async def ainvoke(self, input, config=None, **kwargs):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ —Å rate limiting"""
        self._enforce_rate_limit()
        return await self.llm.ainvoke(input, config=config, **kwargs)
    
    def batch(self, inputs, config=None, **kwargs):
        """Batch –≤—ã–∑–æ–≤ —Å rate limiting"""
        results = []
        for input_item in inputs:
            self._enforce_rate_limit()
            results.append(self.llm.invoke(input_item, config=config, **kwargs))
        return results
    
    async def abatch(self, inputs, config=None, **kwargs):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π batch –≤—ã–∑–æ–≤ —Å rate limiting"""
        results = []
        for input_item in inputs:
            self._enforce_rate_limit()
            results.append(await self.llm.ainvoke(input_item, config=config, **kwargs))
        return results
    
    def _generate(self, prompts, stop=None, run_manager=None, **kwargs):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å rate limiting"""
        self._enforce_rate_limit()
        return self.llm._generate(prompts, stop=stop, run_manager=run_manager, **kwargs)
    
    async def _agenerate(self, prompts, stop=None, run_manager=None, **kwargs):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å rate limiting"""
        self._enforce_rate_limit()
        return await self.llm._agenerate(prompts, stop=stop, run_manager=run_manager, **kwargs)
    
    @property
    def _llm_type(self) -> str:
        return f"rate_limited_{self.llm._llm_type}"

def create_ragas_llm():
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è RAGAS LLM –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: openai (–≤–Ω–µ—à–Ω–∏–π API), huggingface (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å)
    """
    provider = config.RAGAS_LLM_PROVIDER.lower()
    
    if provider == "openai":
        logger.info(f"Creating RAGAS OpenAI LLM: {config.RAGAS_LLM_MODEL}")
        llm_kwargs = {
            "model": config.RAGAS_LLM_MODEL,
            "temperature": 0,
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ rate limit
            "max_retries": config.RAGAS_MAX_RETRIES,
            # –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
            "timeout": config.RAGAS_REQUEST_TIMEOUT,
        }
        # –ü–µ—Ä–µ–¥–∞–µ–º base_url –∏ api_key, –µ—Å–ª–∏ –æ–Ω–∏ –∑–∞–¥–∞–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥–µ
        if config.OPENAI_BASE_URL:
            llm_kwargs["base_url"] = config.OPENAI_BASE_URL
        if config.OPENAI_API_KEY:
            llm_kwargs["api_key"] = config.OPENAI_API_KEY
        
        base_llm = ChatOpenAI(**llm_kwargs)
        
        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ RateLimitedLLM –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–µ—Ä—Ç–∫—É, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–µ–ª–µ–≥–∏—Ä—É–µ—Ç –∞—Ç—Ä–∏–±—É—Ç—ã
        rate_limited_llm = RateLimitedLLM(base_llm, delay=config.RAGAS_REQUEST_DELAY)
        
        logger.info(f"Rate limit protection: max_retries={config.RAGAS_MAX_RETRIES}, timeout={config.RAGAS_REQUEST_TIMEOUT}s, max_wait={config.RAGAS_MAX_WAIT}s, request_delay={config.RAGAS_REQUEST_DELAY}s")
        return rate_limited_llm
    
    elif provider == "huggingface":
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
        import torch
        cuda_available = torch.cuda.is_available()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Å —É—á–µ—Ç–æ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA
        requested_device = config.RAGAS_HUGGINGFACE_LLM_DEVICE
        if requested_device in ["cuda", "auto"] and not cuda_available:
            logger.warning(f"CUDA requested ({requested_device}) but not available. Falling back to CPU.")
            actual_device = "cpu"
        elif requested_device == "auto":
            actual_device = "cuda" if cuda_available else "cpu"
        else:
            actual_device = requested_device
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ CPU
        large_models = ["qwen2.5-7b", "qwen2.5-14b", "saiga2_7b", "saiga2_13b", "llama-7b", "llama-13b"]
        model_name_lower = config.RAGAS_HUGGINGFACE_LLM_MODEL.lower()
        is_large_model = any(lm in model_name_lower for lm in large_models)
        
        if actual_device == "cpu" and is_large_model:
            logger.warning(
                f"‚ö†Ô∏è  Large model '{config.RAGAS_HUGGINGFACE_LLM_MODEL}' on CPU may require 14-26GB RAM. "
                f"Consider using a smaller model like 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' (~3GB RAM) "
                f"or enable quantization (8bit/4bit) if CUDA is available."
            )
        
        logger.info(f"Creating RAGAS HuggingFace LLM: {config.RAGAS_HUGGINGFACE_LLM_MODEL} on {actual_device} (requested: {requested_device}, CUDA available: {cuda_available})")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ model_kwargs –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç quantization
        model_kwargs = {}
        quantization_config = None
        
        # –î–æ–±–∞–≤–ª—è–µ–º quantization, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π API —á–µ—Ä–µ–∑ BitsAndBytesConfig)
        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: quantization —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ CUDA, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–≤–µ—Ä—è–µ–º actual_device
        if config.RAGAS_HUGGINGFACE_LLM_QUANTIZATION == "4bit":
            if actual_device == "cpu":
                logger.warning("4-bit quantization requires CUDA, but device is CPU. Loading without quantization.")
            else:
                try:
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype="float16",
                    )
                    logger.info("Using 4-bit quantization for RAGAS LLM")
                except Exception as e:
                    logger.warning(f"Failed to create 4-bit quantization config: {e}. Loading without quantization.")
        elif config.RAGAS_HUGGINGFACE_LLM_QUANTIZATION == "8bit":
            if actual_device == "cpu":
                logger.warning("8-bit quantization requires CUDA, but device is CPU. Loading without quantization.")
            else:
                try:
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True,
                    )
                    logger.info("Using 8-bit quantization for RAGAS LLM")
                except Exception as e:
                    logger.warning(f"Failed to create 8-bit quantization config: {e}. Loading without quantization.")
        else:
            # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º float32, –¥–ª—è GPU - float16
            if actual_device == "cpu":
                model_kwargs["torch_dtype"] = "float32"
            else:
                model_kwargs["torch_dtype"] = "float16"
        
        # –î–æ–±–∞–≤–ª—è–µ–º quantization_config –≤ model_kwargs, –µ—Å–ª–∏ –æ–Ω —Å–æ–∑–¥–∞–Ω
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º device_map –¥–ª—è –º–æ–¥–µ–ª–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º actual_device –≤–º–µ—Å—Ç–æ config)
        device_map = actual_device if actual_device != "auto" else ("cuda" if cuda_available else "cpu")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º device –¥–ª—è pipeline (–µ—Å–ª–∏ device_map="auto", –∏—Å–ø–æ–ª—å–∑—É–µ–º None –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
        if device_map == "auto":
            pipeline_device = None  # Pipeline —Å–∞–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        elif device_map == "cuda":
            pipeline_device = 0
        else:
            pipeline_device = -1  # CPU
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
        logger.info(f"Loading model {config.RAGAS_HUGGINGFACE_LLM_MODEL}...")
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ –∏–º–µ–Ω–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
            model_name_lower = config.RAGAS_HUGGINGFACE_LLM_MODEL.lower()
            
            # –î–ª—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–∞–∑–µ Llama (Saiga) –∏—Å–ø–æ–ª—å–∑—É–µ–º LlamaTokenizer
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö (Qwen, DeepSeek –∏ —Ç.–¥.) –∏—Å–ø–æ–ª—å–∑—É–µ–º AutoTokenizer
            if "saiga" in model_name_lower or "llama" in model_name_lower:
                try:
                    tokenizer = LlamaTokenizer.from_pretrained(
                        config.RAGAS_HUGGINGFACE_LLM_MODEL,
                        use_fast=False,  # –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –æ–±—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å tiktoken
                        trust_remote_code=True
                    )
                    logger.info("Successfully loaded tokenizer with LlamaTokenizer")
                except Exception as e1:
                    logger.warning(f"LlamaTokenizer failed: {e1}, trying AutoTokenizer")
                    tokenizer = AutoTokenizer.from_pretrained(
                        config.RAGAS_HUGGINGFACE_LLM_MODEL,
                        use_fast=True,  # –î–ª—è –Ω–µ-Llama –º–æ–¥–µ–ª–µ–π –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fast
                        trust_remote_code=True
                    )
                    logger.info("Successfully loaded tokenizer with AutoTokenizer (fallback)")
            else:
                # –î–ª—è Qwen, DeepSeek –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º AutoTokenizer
                tokenizer = AutoTokenizer.from_pretrained(
                    config.RAGAS_HUGGINGFACE_LLM_MODEL,
                    use_fast=True,  # Fast tokenizer –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                    trust_remote_code=True
                )
                logger.info("Successfully loaded tokenizer with AutoTokenizer")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            model_load_kwargs = {
                "trust_remote_code": True,
                **model_kwargs
            }
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º device_map —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ accelerate –¥–æ—Å—Ç—É–ø–µ–Ω –∏ device_map –Ω–µ "cpu"
            # accelerate –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞
            use_device_map = False
            if ACCELERATE_AVAILABLE and device_map not in ["cpu", None, -1]:
                model_load_kwargs["device_map"] = device_map
                use_device_map = True
                logger.info(f"Using device_map={device_map} with accelerate")
            else:
                # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º device_map - –∑–∞–≥—Ä—É–∑–∏–º –º–æ–¥–µ–ª—å –∏ –ø–µ—Ä–µ–º–µ—Å—Ç–∏–º –≤—Ä—É—á–Ω—É—é
                if not ACCELERATE_AVAILABLE:
                    logger.warning("accelerate not available, loading model without device_map")
                logger.info(f"Loading model without device_map (target device: {device_map})")
            
            model = AutoModelForCausalLM.from_pretrained(
                config.RAGAS_HUGGINGFACE_LLM_MODEL,
                **model_load_kwargs
            )
            
            # –ï—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ device_map, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≤—Ä—É—á–Ω—É—é
            if not use_device_map:
                target_device = actual_device  # –ò—Å–ø–æ–ª—å–∑—É–µ–º actual_device, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω
                model = model.to(target_device)
                logger.info(f"Model moved to {target_device}")
            logger.info("Model loaded successfully")
        except MemoryError as e:
            logger.error(f"‚ùå Out of memory while loading model: {e}")
            logger.error(f"Model: {config.RAGAS_HUGGINGFACE_LLM_MODEL}")
            logger.error("üí° Solutions:")
            logger.error("  1. Use a smaller model: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' (~3GB RAM)")
            logger.error("  2. Enable quantization: set RAGAS_HUGGINGFACE_LLM_QUANTIZATION=8bit (requires CUDA)")
            logger.error("  3. Use CUDA if available: set RAGAS_HUGGINGFACE_LLM_DEVICE=cuda")
            logger.error("  4. Close other applications to free RAM")
            raise
        except (OSError, RuntimeError) as e:
            error_str = str(e).lower()
            if "out of memory" in error_str or "memory" in error_str:
                logger.error(f"‚ùå Out of memory error: {e}")
                logger.error(f"Model: {config.RAGAS_HUGGINGFACE_LLM_MODEL}")
                logger.error("üí° Solutions:")
                logger.error("  1. Use a smaller model: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' (~3GB RAM)")
                logger.error("  2. Enable quantization: set RAGAS_HUGGINGFACE_LLM_QUANTIZATION=8bit (requires CUDA)")
                logger.error("  3. Use CUDA if available: set RAGAS_HUGGINGFACE_LLM_DEVICE=cuda")
                logger.error("  4. Close other applications to free RAM")
            else:
                logger.error(f"Error loading model: {e}")
                logger.error("Try using a different model or check if the model requires special configuration")
                logger.error(f"Model: {config.RAGAS_HUGGINGFACE_LLM_MODEL}")
                logger.error("Recommended alternatives: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B (lightweight) or Qwen/Qwen2.5-7B-Instruct (better quality, needs more RAM)")
            raise
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            logger.error("Try using a different model or check if the model requires special configuration")
            logger.error(f"Model: {config.RAGAS_HUGGINGFACE_LLM_MODEL}")
            logger.error("Recommended alternatives: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B (lightweight) or Qwen/Qwen2.5-7B-Instruct (better quality, needs more RAM)")
            raise
        
        # –°–æ–∑–¥–∞–µ–º pipeline
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º pipeline –ª–æ–∫–∞–ª—å–Ω–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å torchvision –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
        from transformers import pipeline
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å device_map (accelerate), –Ω–µ –ø–µ—Ä–µ–¥–∞–µ–º device –≤ pipeline
        # –ò–Ω–∞—á–µ –ø–µ—Ä–µ–¥–∞–µ–º device –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        pipeline_kwargs = {
            "model": model,
            "tokenizer": tokenizer,
            "max_new_tokens": 512,
            "temperature": 0,
            "do_sample": False,  # –î–ª—è evaluation –Ω—É–∂–Ω–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
        }
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å device_map, –Ω–µ –ø–µ—Ä–µ–¥–∞–µ–º device
        # –ò–Ω–∞—á–µ –ø–µ—Ä–µ–¥–∞–µ–º device –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        if not use_device_map:
            pipeline_kwargs["device"] = pipeline_device
            logger.info(f"Creating pipeline with device={pipeline_device}")
        else:
            logger.info("Creating pipeline without device argument (model uses device_map)")
        
        pipe = pipeline("text-generation", **pipeline_kwargs)
        
        # –û–±–µ—Ä—Ç—ã–≤–∞–µ–º –≤ Langchain LLM (HuggingFacePipeline —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –æ–±—ã—á–Ω—ã–π LLM)
        # RAGAS –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ–±—ã—á–Ω—ã–º LLM —á–µ—Ä–µ–∑ LangchainLLMWrapper
        return HuggingFacePipeline(pipeline=pipe)
    
    else:
        raise ValueError(f"Unknown RAGAS LLM provider: {provider}. Use 'openai' or 'huggingface'")

def create_ragas_embeddings():
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è RAGAS embeddings –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: openai, huggingface
    """
    provider = config.RAGAS_EMBEDDING_PROVIDER.lower()
    
    if provider == "openai":
        logger.info(f"Creating RAGAS OpenAI embeddings: {config.RAGAS_EMBEDDING_MODEL}")
        embedding_kwargs = {"model": config.RAGAS_EMBEDDING_MODEL}
        # –ü–µ—Ä–µ–¥–∞–µ–º base_url –∏ api_key, –µ—Å–ª–∏ –æ–Ω–∏ –∑–∞–¥–∞–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥–µ
        if config.OPENAI_BASE_URL:
            embedding_kwargs["base_url"] = config.OPENAI_BASE_URL
        if config.OPENAI_API_KEY:
            embedding_kwargs["api_key"] = config.OPENAI_API_KEY
        return OpenAIEmbeddings(**embedding_kwargs)
    
    elif provider == "huggingface":
        logger.info(f"Creating RAGAS HuggingFace embeddings: {config.RAGAS_HUGGINGFACE_EMBEDDING_MODEL} on {config.RAGAS_HUGGINGFACE_DEVICE}")
        return HuggingFaceEmbeddings(
            model_name=config.RAGAS_HUGGINGFACE_EMBEDDING_MODEL,
            model_kwargs={'device': config.RAGAS_HUGGINGFACE_DEVICE},
            encode_kwargs={'normalize_embeddings': True}
        )
    
    else:
        raise ValueError(f"Unknown RAGAS embedding provider: {provider}. Use 'openai' or 'huggingface'")

def init_ragas_metrics():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAGAS –º–µ—Ç—Ä–∏–∫ (–æ–¥–∏–Ω —Ä–∞–∑)
    
    –ü–æ –æ–±—Ä–∞–∑—Ü—É —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞ (—Ä–∞–∑–¥–µ–ª 5.1)
    """
    global _ragas_metrics, _ragas_run_config, _cached_provider
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä - –µ—Å–ª–∏ –¥–∞, –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
    current_provider = config.RAGAS_LLM_PROVIDER.lower()
    if _ragas_metrics is not None and _cached_provider == current_provider:
        return _ragas_metrics, _ragas_run_config
    
    # –ï—Å–ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–∑–º–µ–Ω–∏–ª—Å—è, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∫—ç—à
    if _cached_provider is not None and _cached_provider != current_provider:
        logger.info(f"RAGAS LLM provider changed from '{_cached_provider}' to '{current_provider}'. Reinitializing metrics...")
        _ragas_metrics = None
        _ragas_run_config = None
    
    logger.info("Initializing RAGAS metrics...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–ª—è Fireworks (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI –ø—Ä–æ–≤–∞–π–¥–µ—Ä)
    if config.RAGAS_LLM_PROVIDER.lower() == "openai":
        if config.OPENAI_BASE_URL and "fireworks" in config.OPENAI_BASE_URL.lower():
            if not config.RAGAS_LLM_MODEL or config.RAGAS_LLM_MODEL == "gpt-4o":
                logger.warning(
                    f"RAGAS_LLM_MODEL is set to '{config.RAGAS_LLM_MODEL}' but using Fireworks API. "
                    f"This may cause 404 errors. Set RAGAS_LLM_MODEL to a Fireworks model (e.g., "
                    f"'accounts/fireworks/models/gpt-oss-120b') in your .env file."
                )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –∏ embeddings –¥–ª—è RAGAS (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π –æ—Ü–µ–Ω–∫–∏)
    langchain_llm = create_ragas_llm()
    
    if config.RAGAS_LLM_PROVIDER.lower() == "openai":
        logger.info(f"RAGAS LLM configured: {config.RAGAS_LLM_MODEL} (provider: openai, base_url: {config.OPENAI_BASE_URL or 'default'})")
    else:
        logger.info(f"RAGAS LLM configured: {config.RAGAS_HUGGINGFACE_LLM_MODEL} (provider: huggingface, device: {config.RAGAS_HUGGINGFACE_LLM_DEVICE})")
    
    langchain_embeddings = create_ragas_embeddings()
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics = [
        Faithfulness(),
        ResponseRelevancy(strictness=1),
        AnswerCorrectness(),
        AnswerSimilarity(),
        ContextRecall(),
        ContextPrecision(),
    ]
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
    # –ï—Å–ª–∏ langchain_llm - —ç—Ç–æ RateLimitedLLM, –ø–µ—Ä–µ–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π LLM –≤ LangchainLLMWrapper
    # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –ø–æ–ª—é llm –≤ pydantic
    # Rate limiting –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ RateLimitedLLM
    if isinstance(langchain_llm, RateLimitedLLM):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π LLM –¥–ª—è LangchainLLMWrapper
        base_llm_for_ragas = langchain_llm.llm
        logger.info("Using base LLM (extracted from RateLimitedLLM) for LangchainLLMWrapper to avoid pydantic field access issues")
        # –°–æ–∑–¥–∞–µ–º wrapper —Å –±–∞–∑–æ–≤—ã–º LLM
        ragas_llm = LangchainLLMWrapper(base_llm_for_ragas)
        # –ó–∞–º–µ–Ω—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π LLM –Ω–∞ RateLimitedLLM, —á—Ç–æ–±—ã rate limiting —Ä–∞–±–æ—Ç–∞–ª
        # LangchainLLMWrapper –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RateLimitedLLM –¥–ª—è –≤—ã–∑–æ–≤–æ–≤
        ragas_llm.llm = langchain_llm
    else:
        ragas_llm = LangchainLLMWrapper(langchain_llm)
    
    ragas_embeddings = LangchainEmbeddingsWrapper(langchain_embeddings)
    
    for metric in metrics:
        if isinstance(metric, MetricWithLLM):
            metric.llm = ragas_llm
        if isinstance(metric, MetricWithEmbeddings):
            metric.embeddings = ragas_embeddings
        run_config = RunConfig()
        metric.init(run_config)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ rate limit –æ—à–∏–±–æ–∫
    run_config = RunConfig(
        max_workers=1,  # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        max_wait=config.RAGAS_MAX_WAIT,  # –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ (–∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
        max_retries=config.RAGAS_MAX_RETRIES  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö (–∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
    )
    logger.info(f"RAGAS RunConfig: max_workers=1, max_wait={config.RAGAS_MAX_WAIT}s, max_retries={config.RAGAS_MAX_RETRIES}")
    
    _ragas_metrics = metrics
    _ragas_run_config = run_config
    _cached_provider = current_provider  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    
    logger.info(f"‚úì RAGAS metrics initialized: {', '.join([m.name for m in metrics])}")
    logger.info(f"‚úì RAGAS LLM Provider: {config.RAGAS_LLM_PROVIDER}")
    if config.RAGAS_LLM_PROVIDER.lower() == "openai":
        logger.info(f"‚úì RAGAS LLM: {config.RAGAS_LLM_MODEL}")
    else:
        logger.info(f"‚úì RAGAS LLM: {config.RAGAS_HUGGINGFACE_LLM_MODEL} (device: {config.RAGAS_HUGGINGFACE_LLM_DEVICE}, quantization: {config.RAGAS_HUGGINGFACE_LLM_QUANTIZATION})")
    logger.info(f"‚úì RAGAS Embedding Provider: {config.RAGAS_EMBEDDING_PROVIDER}")
    if config.RAGAS_EMBEDDING_PROVIDER == "openai":
        logger.info(f"‚úì RAGAS Embedding Model: {config.RAGAS_EMBEDDING_MODEL}")
    else:
        logger.info(f"‚úì RAGAS Embedding Model: {config.RAGAS_HUGGINGFACE_EMBEDDING_MODEL} on {config.RAGAS_HUGGINGFACE_DEVICE}")
    
    return _ragas_metrics, _ragas_run_config

def check_dataset_exists(dataset_name: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ LangSmith
    
    Args:
        dataset_name: –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞
    
    Returns:
        True –µ—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    """
    if not config.LANGSMITH_API_KEY:
        logger.error("LANGSMITH_API_KEY not set")
        return False
    
    try:
        client = Client()
        datasets = list(client.list_datasets(dataset_name=dataset_name))
        return len(datasets) > 0
    except Exception as e:
        logger.error(f"Error checking dataset: {e}")
        return False

def evaluate_dataset(dataset_name: Optional[str] = None) -> Dict[str, Any]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è evaluation RAG —Å–∏—Å—Ç–µ–º—ã
    
    –ü–æ –æ–±—Ä–∞–∑—Ü—É —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞ (—Ä–∞–∑–¥–µ–ª 5.2):
    1. –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ LangSmith —Å blocking=False –∏ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
    2. RAGAS batch evaluation
    3. –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫ –∫–∞–∫ feedback –≤ LangSmith
    
    Args:
        dataset_name: –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
    
    Returns:
        dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ evaluation
    """
    if not config.LANGSMITH_API_KEY:
        raise ValueError("LANGSMITH_API_KEY not set. Cannot run evaluation.")
    
    if dataset_name is None:
        dataset_name = config.LANGSMITH_DATASET
    
    logger.info(f"Starting evaluation for dataset: {dataset_name}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    if not check_dataset_exists(dataset_name):
        raise ValueError(f"Dataset '{dataset_name}' not found in LangSmith")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
    ragas_metrics, ragas_run_config = init_ragas_metrics()
    
    client = Client()
    
    # ========== –®–∞–≥ 1: –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ==========
    logger.info("\n[1/3] Running experiment and collecting data...")
    
    # –°–æ–∑–¥–∞–µ–º target —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –Ω–∞—à–µ–≥–æ RAG
    def target(inputs: dict) -> dict:
        """Target —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è evaluation"""
        question = inputs["question"]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é RAG —Ü–µ–ø–æ—á–∫—É
        # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å (–±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è evaluation)
        from langchain_core.messages import HumanMessage
        result = rag.get_rag_chain().invoke({"messages": [HumanMessage(content=question)]})
        
        return {
            "answer": result["answer"],
            "documents": result["documents"]
        }
    
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è evaluate
    questions = []
    answers = []
    contexts_list = []
    ground_truths = []
    run_ids = []
    
    # evaluate() —Å blocking=False –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ç–µ—Ä–∞—Ç–æ—Ä
    for result in client.evaluate(
        target,
        data=dataset_name,
        evaluators=[],
        experiment_prefix="rag-evaluation",
        metadata={
            "approach": "RAGAS batch evaluation + LangSmith feedback",
            "model": config.MODEL,
            "embedding_model": config.EMBEDDING_MODEL,
        },
        blocking=False,
    ):
        run = result["run"]
        example = result["example"]
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        question = run.inputs.get("question", "")
        answer = run.outputs.get("answer", "")
        documents = run.outputs.get("documents", [])
        contexts = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in documents]
        ground_truth = example.outputs.get("answer", "") if example else ""
        
        questions.append(question)
        answers.append(answer)
        contexts_list.append(contexts)
        ground_truths.append(ground_truth)
        run_ids.append(str(run.id))
    
    logger.info(f"Experiment completed, collected {len(questions)} examples")
    
    # ========== –®–∞–≥ 2: RAGAS evaluation ==========
    logger.info("\n[2/3] Running RAGAS evaluation...")
    
    # –°–æ–∑–¥–∞–µ–º Dataset –¥–ª—è RAGAS
    ragas_dataset = Dataset.from_dict({
        "question": questions,
        "answer": answers,
        "contexts": contexts_list,
        "ground_truth": ground_truths
    })
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º evaluation —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    try:
        ragas_result = evaluate(
            ragas_dataset,
            metrics=ragas_metrics,
            run_config=ragas_run_config,
        )
        
        ragas_df = ragas_result.to_pandas()
        
        logger.info("RAGAS evaluation completed")
    except Exception as e:
        error_msg = str(e)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ rate limit –æ—à–∏–±–∫–æ–π
        if "429" in error_msg or "rate limit" in error_msg.lower() or "RateLimitError" in str(type(e)):
            logger.error(f"‚ùå Rate limit error during RAGAS evaluation: {e}")
            logger.error("üí° Solutions:")
            logger.error(f"  1. Wait a few minutes and try again (current max_wait={config.RAGAS_MAX_WAIT}s)")
            logger.error(f"  2. Increase RAGAS_MAX_RETRIES in .env (current: {config.RAGAS_MAX_RETRIES})")
            logger.error(f"  3. Increase RAGAS_MAX_WAIT in .env (current: {config.RAGAS_MAX_WAIT}s)")
            logger.error("  4. Consider using a local HuggingFace model (RAGAS_LLM_PROVIDER=huggingface)")
            logger.error("  5. Reduce the dataset size or split evaluation into smaller batches")
        else:
            logger.error(f"‚ùå Error during RAGAS evaluation: {e}")
        raise
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
    metrics_summary = {}
    for metric in ragas_metrics:
        if metric.name in ragas_df.columns:
            avg_score = ragas_df[metric.name].mean()
            metrics_summary[metric.name] = avg_score
            logger.info(f"  {metric.name}: {avg_score:.3f}")
    
    # ========== –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ feedback –≤ LangSmith ==========
    logger.info("\n[3/3] Uploading feedback to LangSmith...")
    
    for idx, run_id in enumerate(run_ids):
        row = ragas_df.iloc[idx]
        
        for metric in ragas_metrics:
            if metric.name in row:
                score = row[metric.name]
                client.create_feedback(
                    run_id=run_id,
                    key=metric.name,
                    score=float(score),
                    comment=f"RAGAS metric: {metric.name}"
                )
    
    logger.info(f"Feedback uploaded ({len(run_ids)} runs)")
    
    return {
        "dataset_name": dataset_name,
        "num_examples": len(questions),
        "metrics": metrics_summary,
        "ragas_result": ragas_result,
        "run_ids": run_ids
    }

